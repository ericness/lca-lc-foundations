{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(\n",
    "        asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy\n",
    "    ):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "\n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(model=\"gpt-5-nano\", tools=tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")\n",
    "        ]\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='3dacce9c-c451-46f8-b45a-caac13ec220b'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 271, 'total_tokens': 427, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7qgBsGCBrPXoFc1paEmsy1eYIm0v', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c49b5-a1c5-7981-9d48-4dae51419692-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_w1sVU9Kn8JtehEr2uXxXv351', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 271, 'output_tokens': 156, 'total_tokens': 427, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"response_time\": 0.7,\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.93206495,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two\",\\n      \"score\": 0.8627572,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters) library. import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\";import { createAgent } from \\\\\"langchain\\\\\"; import { createAgent } from  \\\\\"langchain\\\\\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \\\\\"stdio\\\\\", // Local subprocess communication transport:  \\\\\"stdio\\\\\", // Local subprocess communication command: \\\\\"node\\\\\", command:  \\\\\"node\\\\\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\\\\\"/path/to/math_server.js\\\\\"], args: [\\\\\"/path/to/math_server.js\\\\\"], }, }, weather: { weather: { transport: \\\\\"http\\\\\", // HTTP-based remote server transport:  \\\\\"http\\\\\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \\\\\"http://localhost:8000/mcp\\\\\", url: \\\\\"http://localhost:8000/mcp\\\\\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.\",\\n      \"score\": 0.8548001,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters - GitHub\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.8380581,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.8088086,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"request_id\": \"758e6f29-e7c5-4b40-81b6-4d44de2aaafd\"\\n}', 'id': 'lc_0ce5ea55-619b-422c-a21d-1e02fc8cee85'}], name='search_web', id='1521d4dd-d0fb-47e0-8d09-297b22dc62f1', tool_call_id='call_w1sVU9Kn8JtehEr2uXxXv351', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'response_time': 0.7, 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.93206495, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': 'This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two', 'score': 0.8627572, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/javascript/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': '[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`@langchain/mcp-adapters`](https://github.com/langchain-ai/langchainjs/tree/main/libs/langchain-mcp-adapters) library. import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { ChatAnthropic } from \"@langchain/anthropic\"; import { ChatAnthropic } from \"@langchain/anthropic\";import { createAgent } from \"langchain\"; import { createAgent } from  \"langchain\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \"stdio\", // Local subprocess communication transport:  \"stdio\", // Local subprocess communication command: \"node\", command:  \"node\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\"/path/to/math_server.js\"], args: [\"/path/to/math_server.js\"], }, }, weather: { weather: { transport: \"http\", // HTTP-based remote server transport:  \"http\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \"http://localhost:8000/mcp\", url: \"http://localhost:8000/mcp\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.', 'score': 0.8548001, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters - GitHub', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.8380581, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters - NPM', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.8088086, 'raw_content': None}], 'request_id': '758e6f29-e7c5-4b40-81b6-4d44de2aaafd'}}}),\n",
      "              AIMessage(content='Here’s a concise overview of the LangChain MCP Adapters library and how it fits into LangChain, LangGraph, and MCP.\\n\\nWhat it is\\n- A set of adapters that let you connect to Model Context Protocol (MCP) tool servers and use their tools inside LangChain/LangGraph workflows.\\n- It converts MCP tools into LangChain-compatible tools, enabling multi-server access and easy integration with LangChain agents and graphs.\\n\\nKey concepts\\n- MCP (Model Context Protocol): A standard for exposing tools and context to LLMs via tool servers. MCP servers publish tools that can be consumed by clients.\\n- MCP Adapters: The LangChain library that wraps MCP tools as LangChain tools, so agents and graphs can call them just like native LangChain tools.\\n- Multi-server access: You can connect to multiple MCP servers at once and pull in tools from all of them, then use them in a single agent or graph.\\n\\nWhat you can do with it\\n- Load tools from one or more MCP servers without managing your own MCP client logic.\\n- Convert MCP tools into LangChain-compatible tools automatically, so you can use them with LangChain agents, LangGraph graphs, and LangSmith workflows.\\n- Configure per-server or global timeouts, error handling, and tool naming behavior (e.g., whether to prefix tool names with the server name).\\n- For secure MCP servers, support for an authProvider to handle OAuth 2.0, etc.\\n\\nLanguages and ecosystems\\n- JavaScript/TypeScript: The official LangChain MCP Adapters JS library is available as @langchain/mcp-adapters. You can instantiate a MultiServerMCPClient, load tools, and integrate them with LangChain agents and workflows.\\n- Python: There is a Python variant (langchain_mcp_adapters) with a MultiServerMCPClient for use with Python-based LangChain tooling and agents.\\n\\nWhere to learn more (official docs and repo)\\n- JavaScript docs (LangChain JS): https://docs.langchain.com/oss/javascript/langchain/mcp\\n- Python docs (LangChain Python): https://reference.langchain.com/python/langchain_mcp_adapters/\\n- Changelog/overview: MCP Adapters for LangChain and LangGraph (high-level goals and benefits): https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\\n- GitHub repository (source code and examples): https://github.com/langchain-ai/langchain-mcp-adapters\\n- NPM package page (JS usage, options, and config): https://www.npmjs.com/package/@langchain/mcp-adapters\\n\\n quick-start ideas (high level)\\n- JavaScript/TypeScript:\\n  - Install @langchain/mcp-adapters\\n  - Create a MultiServerMCPClient with your MCP servers (e.g., one for math, one for weather)\\n  - Load tools via client.getTools()\\n  - Pass those tools to a LangChain agent or graph as you would with native tools\\n- Python:\\n  - Install langchain_mcp_adapters\\n  - Create a MultiServerMCPClient with your MCP servers\\n  - Call get_tools() and wire the tools into a LangChain agent or graph (e.g., with create_agent)\\n\\nIf you’d like, tell me:\\n- Which language you’re using (JS/TS vs. Python)\\n- The kind of application you’re building (e.g., an agent that uses MCP tools from multiple servers, a LangGraph workflow, etc.)\\n- Any MCP servers you plan to connect to (types of tools you’ll use)\\n\\nI can give you a concrete minimal code snippet tailored to your stack.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2354, 'prompt_tokens': 1957, 'total_tokens': 4311, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1600, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7qgFEYDrpbqdeXnwDw4tyIllgAzy', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c49b5-b5c0-76f0-b092-6348c32898fc-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1957, 'output_tokens': 2354, 'total_tokens': 4311, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1600}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\"mcp-server-time\", \"--local-timezone=America/New_York\"],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='9467c24c-7a5c-4d57-a22a-f91059e80644'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 219, 'prompt_tokens': 296, 'total_tokens': 515, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7qgfujpZlDMCMlBMQeNbQIvIwlgx', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c49b6-182f-7860-ad96-b2a2adc17310-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_PYGBFVimygEVC8td9xqF0UHG', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 296, 'output_tokens': 219, 'total_tokens': 515, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-02-10T17:40:00-05:00\",\\n  \"day_of_week\": \"Tuesday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_e7f13d19-6507-41b7-b048-4e670904ab00'}], name='get_current_time', id='57991b89-41b2-4d95-82a6-494e94654185', tool_call_id='call_PYGBFVimygEVC8td9xqF0UHG'),\n",
      "              AIMessage(content='Current time in New York (America/New_York): Tuesday, February 10, 2026, 5:40 PM (EST, UTC-5). \\n\\nWould you like me to convert this to another timezone? If so, tell me which one.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 446, 'prompt_tokens': 379, 'total_tokens': 825, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7qgjhnUhrndOnhmDhv7GkDbSyXd6', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c49b6-2841-7f32-b773-a32e209c48e6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 379, 'output_tokens': 446, 'total_tokens': 825, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke({\"messages\": [question]})\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-foundations",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
